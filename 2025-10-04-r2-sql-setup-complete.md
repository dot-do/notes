# R2 SQL Setup and Benchmark Infrastructure Complete

**Date:** 2025-10-04
**Status:** ‚úÖ Ready for Pipeline Setup and Testing
**Purpose:** TB-scale MDXLD backlink queries with <100ms latency

## Summary

Successfully completed the infrastructure and tooling setup for benchmarking Cloudflare R2 SQL against D1 and ClickHouse for TB-scale MDXLD graph database queries. R2 SQL is brand new (announced ~3 days ago), so this work involved significant research and pioneering implementation.

## What is R2 SQL?

**Cloudflare R2 SQL** is a serverless, distributed query engine for analyzing petabyte-scale data in R2 object storage using Apache Iceberg table format.

**Key Features:**
- ‚úÖ Petabyte-scale storage capacity
- ‚úÖ Distributed query execution across Cloudflare's global network
- ‚úÖ Apache Iceberg table format with Parquet files
- ‚úÖ Streaming query planning (low latency start)
- ‚úÖ Multi-layer metadata pruning for performance
- ‚úÖ Column-selective reads via Parquet

**Limitations:**
- ‚ö†Ô∏è Open Beta (very new, limited documentation)
- ‚ö†Ô∏è No JOINs or aggregations (but we don't need them!)
- ‚ö†Ô∏è ORDER BY only works on partition keys
- ‚ö†Ô∏è Query interface: Wrangler CLI only (no HTTP API yet)

## Use Case: MDXLD Backlink Queries

### Data Model

**Things** (content):
- `ns` + `id` = URL (e.g., `github.com/dot-do/api`)
- `data` = YAML frontmatter (JSON) for filtering
- `content` = Full markdown with YAML

**Relationships** (backlink index):
- `fromNs` + `fromId` ‚Üí `toNs` + `toId`
- Purpose: Reverse index for "What links TO this page?"
- Auto-generated by parsing markdown links

### Query Pattern

```sql
SELECT fromNs, fromId, fromType, predicate, data
FROM relationships
WHERE toNs = 'github.com' AND toId = '/dot-do/api'
ORDER BY fromNs, fromId
LIMIT 1000
```

**R2 SQL Compatibility:**
- ‚úÖ Single table SELECT - SUPPORTED
- ‚úÖ WHERE clause with = operators - SUPPORTED
- ‚úÖ ORDER BY - SUPPORTED (with partition key)
- ‚úÖ LIMIT - SUPPORTED (1-10,000 range)
- ‚ùå JOINs - NOT NEEDED (single table)
- ‚ùå GROUP BY - NOT NEEDED (no aggregation)

**Conclusion:** R2 SQL CAN execute our backlink query pattern!

## Infrastructure Created

### 1. R2 Bucket with Data Catalog

```bash
# Bucket created
npx wrangler r2 bucket create mdxld-graph

# Data Catalog enabled
npx wrangler r2 bucket catalog enable mdxld-graph

# Result:
Catalog URI: https://catalog.cloudflarestorage.com/b6641681fe423910342b9ffa1364c76d/mdxld-graph
Warehouse: b6641681fe423910342b9ffa1364c76d_mdxld-graph
```

### 2. Schema Definition

**File:** `scripts/r2-sql-relationships-schema.json`

Defines 9 fields for the relationships table:
- fromNs, fromId, fromType (source)
- predicate (relationship type)
- toNs, toId, toType (target)
- data (JSON), createdAt (timestamp)

### 3. Setup Script

**File:** `scripts/r2-sql-setup.sh`

Automates Pipeline creation:
1. Creates Sink (R2 Data Catalog sink ‚Üí bucket/namespace/table)
2. Creates Stream (HTTP endpoint for ingestion)
3. Creates Pipeline (SQL query: stream ‚Üí sink)

**Configuration:**
- Format: Parquet
- Compression: zstd
- Row Group Size: 128MB
- Roll Interval: 300 seconds

**Usage:**
```bash
export R2_SQL_AUTH_TOKEN=<your-token>
./scripts/r2-sql-setup.sh
```

## Sample Data Created

### Sample MDXLD Content

**File:** `scripts/sample-mdxld-data.ts`

Generates realistic MDXLD test data:
- **3 GitHub repositories** (api, db, workers)
- **3 documentation pages** (api, workers, database)
- **2 blog posts** (microservices architecture, R2 SQL)
- **37 relationships** auto-extracted from markdown links

**Data Structure:**
```typescript
interface Thing {
  ns: string        // Domain (e.g., "github.com")
  id: string        // Path (e.g., "/dot-do/api")
  type: string      // Type (e.g., "Repository", "Documentation")
  data: Record<string, any>  // YAML frontmatter as JSON
  content: string   // Full markdown with YAML
}

interface Relationship {
  fromNs: string
  fromId: string
  fromType: string
  predicate: string  // "links_to", "depends_on", etc.
  toNs: string
  toId: string
  toType: string
  data?: Record<string, any>
  createdAt: string  // ISO 8601
}
```

**Test Run:**
```bash
R2_SQL_STREAM_URL=dummy pnpm tsx scripts/ingest-to-r2-sql.ts --dry-run
```

**Output:**
```
‚úÖ Generated 37 relationships
   Things: 8
   Types: {"Repository":3,"Documentation":3,"BlogPost":2}
   Namespaces: github.com, docs.do, blog.do
```

## Ingestion Script

**File:** `scripts/ingest-to-r2-sql.ts`

Sends relationships to R2 SQL Pipeline:
- Batches data (default: 100 records per batch)
- HTTP POST to stream endpoint
- Tracks success/error counts
- Provides next steps for querying

**Usage:**
```bash
export R2_SQL_STREAM_URL=https://your-stream-endpoint.workers.dev
pnpm tsx scripts/ingest-to-r2-sql.ts
```

## R2 SQL Adapter

**File:** `packages/graph-api/src/adapters/r2sql.ts`

Updated adapter with accurate R2 SQL architecture:

**Key Changes:**
1. **Documented Architecture**
   - Writes via Pipelines HTTP endpoint
   - Reads via Wrangler CLI (no HTTP API yet)
   - Storage in Parquet files with Iceberg metadata

2. **Write Operations**
   ```typescript
   async function writeRelationshipsToR2SQL(
     connection: R2SQLConnection,
     relationships: R2SQLRelationship[]
   ): Promise<{ success: boolean; error?: string }>
   ```

3. **Read Operations (Mock)**
   - Logs Wrangler command that would execute
   - Returns empty results for now
   - Production requires Wrangler CLI or proxy worker

4. **Schema Initialization**
   - Documents that tables are created via Pipelines, not SQL DDL
   - Provides setup instructions
   - No-op implementation (API compatibility)

**Connection Interface:**
```typescript
interface R2SQLConnection {
  accountId: string       // Cloudflare account ID
  apiToken: string        // R2 Data Catalog permissions
  bucketName: string      // R2 bucket with Data Catalog
  warehouseName: string   // Usually accountId_bucketName
  streamUrl?: string      // Pipelines stream endpoint
}
```

## Benchmark Suite

**File:** `scripts/benchmark-backlinks.ts`

Comprehensive benchmarking tool for comparing backends:

**Backends Supported:**
1. **D1** (SQLite) - Implemented ‚úÖ
2. **R2 SQL** (Apache Iceberg) - Implemented ‚úÖ
3. **ClickHouse** (Workers Analytics) - Placeholder ‚ö†Ô∏è

**Metrics Collected:**
- Cold start latency (first query)
- Warm cache latency (subsequent queries)
- Min, Max, Avg, P50, P95, P99
- Throughput (queries per second)
- Error rate

**Usage:**
```bash
# Benchmark D1 only
pnpm tsx scripts/benchmark-backlinks.ts --backend d1

# Benchmark R2 SQL only
export R2_SQL_AUTH_TOKEN=<token>
pnpm tsx scripts/benchmark-backlinks.ts --backend r2sql

# Benchmark all backends
pnpm tsx scripts/benchmark-backlinks.ts --all

# Custom parameters
pnpm tsx scripts/benchmark-backlinks.ts \
  --backend d1 \
  --ns github.com \
  --id /dot-do/api \
  --iterations 100 \
  --warmup 10
```

**Output Format:**
```
BENCHMARK RESULTS
===================

D1:
  Cold Start:   125.00ms
  Warm Cache:   88.00ms
  Min:          88.00ms
  Max:          125.00ms
  Avg:          95.50ms
  P50:          92.00ms
  P95:          118.00ms
  P99:          123.00ms
  Throughput:   10.47 queries/sec
  Total Time:   4775.00ms
  Iterations:   50
  Errors:       0

R2 SQL:
  Cold Start:   ???ms
  Warm Cache:   ???ms
  ...

COMPARISON:
  üèÜ D1           95.50ms (1.00x)
     R2 SQL       ???ms   (?.??x)
     ClickHouse   ???ms   (?.??x)
```

## Research Documentation

**File:** `docs/R2-SQL-RESEARCH.md`

Comprehensive research document covering:
- R2 SQL overview and capabilities
- MDXLD data model and query patterns
- SQL support analysis (what's supported vs. what we need)
- Setup process (bucket, catalog, pipeline)
- Performance considerations
- Comparison: R2 SQL vs ClickHouse
- Critical questions to answer
- Next steps and phases

**Key Findings:**
1. R2 SQL CAN execute our backlink query pattern
2. No JOINs or aggregations needed (single-table queries)
3. Partition key strategy: `toNs, toId` for optimal pruning
4. Open Beta status means limited documentation and no benchmarks
5. Query latency unknown - need to test

## Cloudflare Documentation

**File:** `docs/cloudflare-llms-full.txt`

Downloaded complete Cloudflare developer platform documentation:
- Size: 2.2MB
- Lines: 43,929
- Source: https://developers.cloudflare.com/llms-full.txt

Note: R2 SQL not yet included (too new)

## Files Created/Modified

### Created:
1. `scripts/r2-sql-setup.sh` - Pipeline setup automation
2. `scripts/r2-sql-relationships-schema.json` - Iceberg schema
3. `scripts/sample-mdxld-data.ts` - Sample data generation
4. `scripts/ingest-to-r2-sql.ts` - Data ingestion script
5. `scripts/benchmark-backlinks.ts` - Benchmark suite
6. `docs/R2-SQL-RESEARCH.md` - Comprehensive research
7. `docs/cloudflare-llms-full.txt` - Cloudflare docs
8. `notes/2025-10-04-r2-sql-setup-complete.md` - This document

### Modified:
1. `packages/graph-api/src/adapters/r2sql.ts` - Updated with accurate architecture

## Next Steps

### Immediate (Manual Steps Required)

1. **Create API Token**
   - Visit: https://dash.cloudflare.com/profile/api-tokens
   - Create Custom Token
   - Permissions: Account ‚Üí R2 Data Catalog ‚Üí Read & Write
   - Export: `export R2_SQL_AUTH_TOKEN=<token>`

2. **Run Pipeline Setup**
   ```bash
   export CLOUDFLARE_ACCOUNT_ID=b6641681fe423910342b9ffa1364c76d
   export R2_SQL_AUTH_TOKEN=<your-token>
   ./scripts/r2-sql-setup.sh
   ```

3. **Get Stream URL**
   ```bash
   npx wrangler pipelines streams list
   # Find "mdxld_relationships_stream"
   # Copy endpoint URL
   export R2_SQL_STREAM_URL=<url>
   ```

### Phase 1: Basic Testing

1. **Import Sample Data**
   ```bash
   pnpm tsx scripts/ingest-to-r2-sql.ts
   ```

2. **Verify Data Written**
   ```bash
   # Check R2 bucket for Parquet files
   npx wrangler r2 object list mdxld-graph
   ```

3. **Run First Query**
   ```bash
   export WRANGLER_R2_SQL_AUTH_TOKEN=$R2_SQL_AUTH_TOKEN
   npx wrangler r2 sql query "b6641681fe423910342b9ffa1364c76d_mdxld-graph" \
     "SELECT * FROM default.relationships LIMIT 10"
   ```

4. **Test Backlink Query**
   ```bash
   npx wrangler r2 sql query "b6641681fe423910342b9ffa1364c76d_mdxld-graph" \
     "SELECT fromNs, fromId, fromType, predicate FROM default.relationships WHERE toNs = 'github.com' AND toId = '/dot-do/api'"
   ```

### Phase 2: Benchmark

1. **Run D1 Baseline**
   ```bash
   pnpm tsx scripts/benchmark-backlinks.ts --backend d1 --iterations 50
   ```

2. **Run R2 SQL Benchmark**
   ```bash
   pnpm tsx scripts/benchmark-backlinks.ts --backend r2sql --iterations 50
   ```

3. **Compare Results**
   ```bash
   pnpm tsx scripts/benchmark-backlinks.ts --all --iterations 100
   ```

### Phase 3: Scale Testing

1. Import larger datasets (5K, 50K, 500K relationships)
2. Re-measure latency at each scale
3. Test partition key optimizations
4. Identify degradation patterns

### Phase 4: Decision

1. Compare D1 vs R2 SQL results
2. Evaluate ClickHouse (if feasible to set up)
3. Document findings
4. Make recommendation

## Critical Questions to Answer

1. **Query Latency**: Can R2 SQL achieve <100ms for backlink queries?
2. **Partition Strategy**: Does `toNs, toId` partitioning improve performance?
3. **Write Performance**: How fast can we ingest relationships?
4. **Scale Behavior**: Does latency degrade with TB-scale data?
5. **Cost**: What will pricing be after beta?
6. **Production Readiness**: Is R2 SQL stable enough for production?

## Comparison: R2 SQL vs D1 vs ClickHouse

| Feature | R2 SQL | D1 | ClickHouse |
|---------|--------|----|-----------|
| **Status** | Open Beta | Production | Production |
| **Scale** | Petabytes | 10GB/database | Billions of rows |
| **Query Latency** | Unknown | 88-122ms | Sub-10ms |
| **SQL Support** | Limited | Full SQLite | Full SQL |
| **Our Query** | ‚úÖ Supported | ‚úÖ Supported | ‚úÖ Supported |
| **Partition Keys** | Required for ORDER BY | Not needed | Optional |
| **Documentation** | Minimal | Extensive | Extensive |
| **Cost** | Free (beta) | $3.50/mo (ONET) | $5-25/mo |
| **Integration** | Wrangler CLI | Native binding | HTTP API |
| **Global Reads** | Yes | Yes | Limited |

## D1 Performance Baseline

From previous testing (notes/2025-10-04-d1-graph-database-complete.md):

**Query Performance:**
- Inbound relationships: 122ms (2 records)
- Outbound relationships: 113ms (6 records)
- Type filtering: 88ms (5 records)

**Import Performance:**
- 50 records total: 4,348ms (87ms average)
- Occupations: 173ms per record
- Skills: 60ms per record
- Relationships: 86ms per record

**Cost Estimate (ONET dataset):**
- Storage: $0.08/month
- Writes: $0.40/month
- Reads: $3.00/month
- **Total: $3.50/month**

## Open Questions

1. Can we partition on multiple columns in R2 SQL/Iceberg? (Need to test)
2. Is there a Worker-native API or only Wrangler CLI? (CLI only for now)
3. Can we use Pipelines API from Workers? (Yes, stream endpoint is HTTP)
4. What's the batch write performance? (Need to test)
5. Is there query caching or connection pooling? (Unknown)
6. Will R2 SQL be faster than D1 for our use case? (Need benchmark)

## References

- **R2 SQL Docs**: https://developers.cloudflare.com/r2-sql/
- **Deep Dive Blog**: https://blog.cloudflare.com/r2-sql-deep-dive/
- **Data Platform**: https://blog.cloudflare.com/cloudflare-data-platform/
- **Limitations**: https://developers.cloudflare.com/r2-sql/reference/limitations-best-practices/
- **Getting Started**: https://developers.cloudflare.com/r2-sql/get-started/

## Status

**Current Phase:** Infrastructure Complete ‚úÖ
**Next Phase:** Pipeline Setup and First Query Test
**Timeline:** 1-2 hours for setup, 2-4 hours for benchmarking
**Confidence:** Medium (very new product, limited documentation)

## Conclusion

The R2 SQL infrastructure and tooling are ready for testing. Once the API token is created and the Pipeline is set up, we can:

1. Import sample MDXLD data
2. Run first backlink queries
3. Benchmark against D1
4. Determine if R2 SQL can achieve <100ms latency
5. Make recommendation for production architecture

**Key Insight:** R2 SQL's limited SQL support is NOT a blocker for our use case. Our single-table backlink queries are well within R2 SQL's capabilities.

**Next Critical Step:** Create API token and run Pipeline setup script to test actual performance.

---

**Last Updated:** 2025-10-04
**Status:** Infrastructure Complete, Ready for Testing
**Author:** Claude Code
